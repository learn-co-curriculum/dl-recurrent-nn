{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recurrent NNs for sequence modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Why sequence models?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Models like Recurrent neural networks (RNNs) have significantly contributed in areas like Natural Language Processing and speech recognition. Some examples:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- speech recognition: given an input audio file, and the output is a sequence of words.\n",
    "- Music generation: Output y is a sequence, x can be nothing or the first few notes or...\n",
    "- sentiment classification: is the review good or bad? Or how many stars?\n",
    "- Machine translation: google translate\n",
    "- ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at this example. Imagine we have a dataset where each observation is a sentence, and for each sentence we want a model to detect which words are associated with locations. Let's look at the sentence: \n",
    "\n",
    "\"While we were in school, mom went shopping in the square.\"\n",
    "\n",
    "This is a sentence with 11 words, or put differently, a sequence with 11 elements. If this is the first sentence in our entire input space, we would denote it $ x^{(1)}= (x^{(1)<1>},...,x^{(1)<11>})$.\n",
    "\n",
    "$y^{(1)}$ would then be: $(0,0,0,0,1,0,0,0,0,0,1)$, with 11 elements, $(y^{(1)<1>},...,y^{(1)<11>})$.\n",
    "\n",
    "The sentence has eleven words, $T_x^{(1)} = T_y^{(1)} = 11$. As $T_x$ and $T_y$ will differ depending on the sentence length, they need the subscript (i) too!\n",
    "Note that, this just represents this particular sentence x^{(1)}. Other sentences will have other lengths. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. One-hot representations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- As we've seen before, you use the 10,000 most recurring words in the training set, or look at an online dictionary of commonly used words.\n",
    "- Apply dictionary to our text to create one-hot representations per word\n",
    "- each of the vector is 9,999 times 0 and 1 time 1.\n",
    "- With 11 words in a sentence, 11 one-hot vectors of 10,000 units.\n",
    "- If word not in in your 10,000 words dictionary, you create a new vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. RNN model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have done NLP before, looking at the data sets with bank complaints. Why are \"regular' neural networks sometimes insufficient to deal with NLP and sequence-type problems?\n",
    "- We created vectors with 0s and 1s denoting if certain words are in a given bank complaint, but no information on the sequence of the word is used!\n",
    "- In this example, inputs and outputs can be different lengths for different index numbers (i)\n",
    "- General neural networks cannot learn features across different positions of text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take first word $x^{<1>}$, feed it in an NN layer, and try to predict $\\hat y^{<1>}$. previous words also have an effect on the output for words later in the sequence. We use the weights $w_{ax}, w_{aa}$ and $w_{ya}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](RNN_2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Disadvantage: networks only uses words earlier in the sequence. Eg:\n",
    "\n",
    "In \"Europe, people tend to use square meters instead of square feet.\"\n",
    "\n",
    "The \"meters\" and \"feet\" would have been useful to identify that in this case, square is not a physical location."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$a^{<0>}$ = vector of zeros\n",
    "\n",
    "$a^{<1>} = g(w_{aa}a^{<0>} +w_{ax}x^{<1>}+b_a )$, tanh or relu\n",
    "\n",
    "$\\hat y^{<1>} = g(w_{ya}a^{<1>} + b_y )$, sigmoid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "simpler notation:\n",
    "\n",
    "$a^{<1>} = g(w_{a}[a^{<0>},x^{<1>}]+b_a )$\n",
    "\n",
    "$\\hat y^{<t>} = g(w_{y}a^{<t>}+b_y )$\n",
    "\n",
    "Matrix $w_a=[w_{aa}; w_{ax}]$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"backpropagation through time\" --> Loss function in each vertical in the image above, and then take the sum over all of them, also, right-to=left backpropagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Different types of architectures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Many-to-many --> (location identifyer)\n",
    "- Many-to-one --> text classifyer (good vs bad review)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](RNN_manytoone.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- One-to-many --> music generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](RNN_onetomany.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- many to many, but input and output lengths are different"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](RNN_manytomany.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
