{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recurrent NNs for sequence modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Why sequence models?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Models like Recurrent neural networks (RNNs) have significantly contributed in areas like Natural Language Processing and speech recognition. Some examples:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- speech recognition: given an input audio file, and the output is a sequence of words.\n",
    "- Music generation: Output y is a sequence, x can be nothing or the first few notes or...\n",
    "- sentiment classification: is the review good or bad? Or how many stars?\n",
    "- Machine translation: google translate\n",
    "- ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at this example. Imagine we have a dataset where each observation is a sentence, and for each sentence we want a model to detect which words are associated with locations. Let's look at the sentence: \n",
    "\n",
    "\"While we were in school, mom went shopping in the square.\"\n",
    "\n",
    "This is a sentence with 11 words, or put differently, a sequence with 11 elements. If this is the first sentence in our entire input space, we would denote it $ x^{(1)}= (x^{(1)<1>},...,x^{(1)<11>})$.\n",
    "\n",
    "$y^{(1)}$ would then be: $(0,0,0,0,1,0,0,0,0,0,1)$, with 11 elements, $(y^{(1)<1>},...,y^{(1)<11>})$.\n",
    "\n",
    "The sentence has eleven words, $T_x^{(1)} = T_y^{(1)} = 11$. As $T_x$ and $T_y$ will differ depending on the sentence length, they need the subscript (i) too!\n",
    "Note that, this just represents this particular sentence $x^{(1)}$. Other sentences will have other lengths. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. RNN model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have done NLP before, looking at the data sets with bank complaints. Why are \"regular' neural networks sometimes insufficient to deal with NLP and sequence-type problems?\n",
    "- We created vectors with 0s and 1s denoting if certain words are in a given bank complaint, but no information on the sequence of the word is used!\n",
    "- In this example, inputs and outputs can be different lengths for different index numbers (i)\n",
    "- General neural networks cannot learn features across different positions of text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take first word $x^{<1>}$, feed it in an NN layer, and try to predict $\\hat y^{<1>}$. previous words also have an effect on the output for words later in the sequence. We use the weights $w_{ax}, w_{aa}$ and $w_{ya}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](RNN_2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Disadvantage: networks only uses words earlier in the sequence. Eg:\n",
    "\n",
    "In \"Europe, people tend to use square meters instead of square feet.\"\n",
    "\n",
    "The \"meters\" and \"feet\" would have been useful to identify that in this case, square is not a physical location."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$a^{<0>}$ = vector of zeros\n",
    "\n",
    "$a^{<1>} = g(w_{aa}a^{<0>} +w_{ax}x^{<1>}+b_a )$, tanh or relu\n",
    "\n",
    "$\\hat y^{<1>} = g(w_{ya}a^{<1>} + b_y )$, sigmoid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "simpler notation:\n",
    "\n",
    "$a^{<1>} = g(w_{a}[a^{<0>},x^{<1>}]+b_a )$\n",
    "\n",
    "$\\hat y^{<t>} = g(w_{y}a^{<t>}+b_y )$\n",
    "\n",
    "Matrix $w_a=[w_{aa}; w_{ax}]$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"backpropagation through time\" --> Loss function in each vertical in the image above, and then take the sum over all of them, also, right-to=left backpropagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Different types of architectures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Many-to-many --> (location identifyer)\n",
    "- Many-to-one --> text classifyer (good vs bad review)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](RNN_manytoone.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- One-to-many --> music generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](RNN_onetomany.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- many to many, but input and output lengths are different"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](RNN_manytomany.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sources:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.coursera.org/learn/nlp-sequence-models/lecture/gw1Xw/language-model-and-sequence-generation\n",
    "\n",
    "https://www.coursera.org/learn/nlp-sequence-models/lecture/MACos/sampling-novel-sequences\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Issues in sequence models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Vanishing gradients: plural subject, and maybe much later in the sentence we'll need a plural verb.\n",
    "- Exploding gradients: you'll notice NaNs, solution is gradient clipping (but, exploding gradients are rare in recurrent networks)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Solution for vanishing gradients: Gated Recurrent Unit (GRU)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add a memory cell C. A simplified GRU then looks like this:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "IN a GRU, $a^{<t>} = C^{<t>}$\n",
    "Next, candidate for replacing $C^{<t>}$, which is \n",
    "    \n",
    "$\\tilde C ^{<t>} = \\tanh(w_c[C^{<t-1>}, x^{<t>}]+b_c)$\n",
    "\n",
    "$\\Gamma_u = \\sigma(w_u[c^{<t-1>}, x^{<t>}]+b_u])$\n",
    "--> always 0 or 1\n",
    "\n",
    "$C ^{<t>} =\\Gamma_u * \\tilde C ^{<t>} + (1- \\Gamma_u)* C^{<t-1>}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The $\\Gamma$ is used to decide whether or not to update! If update gate is 0, $C^{<t>}$ will not be updated!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Depending on how many hidden activation values you have, $C^{<t>}, \\tilde C^{<t>}$ and $\\Gamma_u$ will be vectors of values, and the last equation above is "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A FULL GRU adds another gate which tells us how important $C^{<t-1>}$ for the update of $\\tilde C ^{<t>}$. The new equations are:\n",
    "\n",
    "$\\tilde C ^{<t>} = \\tanh(w_c[\\Gamma_r * C^{<t-1>}, x^{<t>}]+b_c)$\n",
    "\n",
    "$\\Gamma_u = \\sigma(w_u[c^{<t-1>}, x^{<t>}]+b_u])$\n",
    "\n",
    "$C ^{<t>} =\\Gamma_u * \\tilde C ^{<t>} + (1- \\Gamma_u)* C^{<t-1>}$\n",
    "\n",
    "$\\Gamma_r = \\sigma(w_r[c^{<t-1>}, x^{<t>}]+b_r])$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.coursera.org/learn/nlp-sequence-models/lecture/agZiL/gated-recurrent-unit-gru"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Solution for vanishing gradients: Long Short Term Memory (LSTM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In LSTM, we do not use that $a^{<t>}=C ^{<t>}$\n",
    "\n",
    "$\\tilde C ^{<t>} = \\tanh(w_c[a^{<t-1>}, x^{<t>}]+b_c)$\n",
    "\n",
    "$\\Gamma_u = \\sigma(w_u[a^{<t-1>}, x^{<t>}]+b_u])$\n",
    "\n",
    "$\\Gamma_f = \\sigma(w_u[c^{<t-1>}, x^{<t>}]+b_f])$\n",
    "\n",
    "$\\Gamma_o = \\sigma(w_o[c^{<t-1>}, x^{<t>}]+b_o])$\n",
    "\n",
    "$\\tilde C ^{<t>} = \\Gamma_u* \\tilde C ^{<t>} + \\Gamma_f* C ^{<t-1>}$ \n",
    "$a ^{<t>} = \\Gamma_o* \\tanh C ^{<t>} $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With $\\Gamma_u, \\Gamma_f$ and $\\Gamma_o$ the update, forget and output gate respectively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Which one to use?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GRU: simpler, and easier to run on a big network\n",
    "\n",
    "LSTM: more powerful, more flexible\n",
    "\n",
    "LSTM is generally considered as the default, but more complex."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 Other networks "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bi-directional RNN: take information about the entire sequence, not just what is preceding!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deep RNNs: several vertical layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Word embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 One-hot representations.\n",
    "\n",
    "- As we've seen before, you use the 10,000 most recurring words in the training set, or look at an online dictionary of commonly used words.\n",
    "- Apply dictionary to our text to create one-hot representations per word\n",
    "- each of the vector is 9,999 times 0 and 1 time 1.\n",
    "- With 11 words in a sentence, 11 one-hot vectors of 10,000 units.\n",
    "- If word not in in your 10,000 words dictionary, you create a new vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--> each word is a thing to itself, and cannot easily generalize across words. eg, relationship between fruits, between locations,... unclear"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Solution: word embeddings! Example:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|      | Duke  | Dutchess | cow | dog |\n",
    "| -----|------ |------- |-----| ----|\n",
    "|Noble |  1    | 1      | 0.03 |  0.01 |\n",
    "|  food | 0.01 | -0.02 | 0.7  | 0.08 |\n",
    "| Gender | -1  |  1   | 0.02 | 0.03 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The words on the left column are \"features\". Now imagine \"duke\", \"dutchess\", \"cow\" and \"dog\" represent locations in our 10,000 most recurreing words vector. If Duke is on the 1265th place, we can create a n-dimensional vector (with n the amount of features) representing the word \"duke\". This way, our algorithm will know that \"cow\" and \"dog\" are more similar than \"cow\" and \"duke\".\n",
    "\n",
    "Word embeddings can also be visualized, using the t-SNE algorithnm to make an n-dimensional space 2D!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"While we were in school, mom went shopping in the square.\"\n",
    "\n",
    "--> if we know that school and square are physical locations, similar words like street, class,... would be recognized more easily. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How do transfer learning from word embeddings? \n",
    "- 1) learn word embeddings for large body of text (or download pre-trained embeddings online)\n",
    "- 2) Transfer embedding to smaller training set\n",
    "- 3) If you want, you can continue finetuning the word embeddings with new data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "word embeddings especially useful when you are doing tasks on small data sets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Properties of word embeddings\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. by taking differences between word embeddings, you can get a better understanding of words that are related to each other. eg. vector difference between boy and girl is similar to the vector difference between duke and dutchess! Only difference is gender. A high degree of similarity between \n",
    "\n",
    "arg max $sim(e_{w}, e_{boy}-e_{duke}+e_{duchess})$, usually cosine similarity is used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|n|   a   | ...  | cow | duke | ... | zucchini|\n",
    "|--|-----|------|------- |-----| ----|-----|\n",
    "|1 | gender|... | 0.03 | -1 |  0.01 | -0.02|\n",
    "|2 |food | ... | -0.02 | 0.7  | 0.08 |0.99|\n",
    "|... | ... |... | ... | ... | ... | ... | \n",
    "|500| furniture |...  | 0.02 | 0.01| ...|-0.03|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "when you multiply one-hot vector representing \"cow\" with embeddingsmatrix, you will basically just keep the column with the embedding representation for cow! In practice, you'll use an embedding layer that simply pulls out a column, because performing these multiplications is much slower when we have huge vectors with all 0's and just one 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 Some examples of language models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.3.1 learning word embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You want to predict the next word in a sequence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. A sentence: get the word embedding vector for each word. Embedding vector has a fixed amount of rows (=number of embeddings).\n",
    "\n",
    "2. Feed all of them into a neural  network. Usual practice is to fix the amount of presceding words to, let's say, 5. If embedding has 500 rows, leads to 2500-dimensional feature vector going into the first layer. End of the network is a softmax.\n",
    "\n",
    "other possibilities: last 3 words and 3 words after, last 1 word, nearby 1 word."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.3.2 word2vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "simpler way to learn embeddings: **skip-gram model**. Context-target pairs. You randomly pick a word to be context word, and then randomly pick a target word withing a specified window from the context word, let's say, 5 words apart.\n",
    "\n",
    "\"The teacher was writing on the blackboard while the principal walked in\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Context: blackboard \n",
    "\n",
    "target: could be principal, while, blackboard,..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to map our context \"c\" (blackboard) to our target \"t\"\n",
    "\n",
    "The model looks like this (with e the embessing : softmax: \n",
    "$P(t|c) = \\displaystyle \\frac{e^{\\Theta^T_te_c}}{\\sum^{10,000}_{j=1}e^{\\Theta^T_je_c}}$\n",
    "\n",
    "loss function: $\\mathcal L (\\hat y, y ) = -\\sum^{10,000}_{i=1}y_i \\log \\hat y_i$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\Theta_t$ is the parameter associated with output t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.coursera.org/learn/nlp-sequence-models/lecture/8CZiw/word2vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Problem: slow, so hierarchical softmax is suggested!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.3.3 Negative sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "word2vec is slow! Try this:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- given a pair of words, try to predict whether or not it is a context target pair (1 or 0). This way, label word combinations with 0 or 1. First actively label pairs of words, then create a supervised learning problem.\n",
    "- logistic regression model: P(y=1|c,t) with c and t the context target pair.\n",
    "- Do a small logistic regression model with about 5-6 content-target pairs (only 1 1, rest is 0 so you deliberately use your negative samples!). Repeat this 10,000 times where each time another content word is chosen. faster than working with a softmax classifier! \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.3.4 Negative sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GloVe: less frequently used, but has enthusiast because so simple!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again c,t: use x_{i,j} to define how often 2 words appear close to each other."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Words embeddings applications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
