{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to word embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Why sequence models?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Models like Recurrent neural networks (RNNs) have significantly contributed in areas like Natural Language Processing and speech recognition. Some examples:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- speech recognition: given an input audio file, and the output is a sequence of words.\n",
    "- Music generation: Output y is a sequence, x can be nothing or the first few notes or...\n",
    "- sentiment classification: is the review good or bad? Or how many stars?\n",
    "- Machine translation: google translate\n",
    "- ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at this example. Imagine we have a dataset where each observation is a sentence, and for each sentence we want a model to detect which words are associated with locations. Let's look at the sentence: \n",
    "\n",
    "\"While we were in school, mom went shopping in the square.\"\n",
    "\n",
    "This is a sentence with 11 words, or put differently, a sequence with 11 elements. If this is the first sentence in our entire input space, we would denote it $ x^{(1)}= (x^{(1)<1>},...,x^{(1)<11>})$.\n",
    "\n",
    "$y^{(1)}$ would then be: $(0,0,0,0,1,0,0,0,0,0,1)$, with 11 elements, $(y^{(1)<1>},...,y^{(1)<11>})$.\n",
    "\n",
    "The sentence has eleven words, $T_x^{(1)} = T_y^{(1)} = 11$. As $T_x$ and $T_y$ will differ depending on the sentence length, they need the subscript (i) too!\n",
    "Note that, this just represents this particular sentence $x^{(1)}$. Other sentences will have other lengths. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a problem with using one-hot vectors: each word is a thing to itself, and cannot easily generalize across words. eg, relationship between fruits, between locations,... unclear"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Intro to word embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 One-hot representations.\n",
    "\n",
    "- As we've seen before, you use the 10,000 most recurring words in the training set, or look at an online dictionary of commonly used words.\n",
    "- Apply dictionary to our text to create one-hot representations per word\n",
    "- each of the vector is 9,999 times 0 and 1 time 1.\n",
    "- With 11 words in a sentence, 11 one-hot vectors of 10,000 units.\n",
    "- If word not in in your 10,000 words dictionary, you create a new vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Solution to one-hot vectors: word embeddings! Example:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|      | Duke  | Dutchess | cow | dog |\n",
    "| -----|------ |------- |-----| ----|\n",
    "|Noble |  1    | 1      | 0.03 |  0.01 |\n",
    "|  food | 0.01 | -0.02 | 0.7  | 0.08 |\n",
    "| Gender | -1  |  1   | 0.02 | 0.03 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The words on the left column are \"features\". Now imagine \"duke\", \"dutchess\", \"cow\" and \"dog\" represent locations in our 10,000 most recurreing words vector. If Duke is on the 1265th place, we can create a n-dimensional vector (with n the amount of features) representing the word \"duke\". This way, our algorithm will know that \"cow\" and \"dog\" are more similar than \"cow\" and \"duke\".\n",
    "\n",
    "Where one-hot vectors are sparse, high-dimensional and hardcoded, word embeddings are dense, lower-dimensional and learned from data (not hardcoded)!.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Word embeddings can also be visualized, using the t-SNE algorithnm to make an n-dimensional space 2D!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"While we were in school, mom went shopping in the square.\"\n",
    "\n",
    "--> if we know that school and square are physical locations, similar words like street, class,... would be recognized more easily. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How do transfer learning from word embeddings? \n",
    "- 1) learn word embeddings for large body of text (or download pre-trained embeddings online)\n",
    "- 2) Transfer embedding to smaller training set\n",
    "- 3) If you want, you can continue finetuning the word embeddings with new data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "word embeddings especially useful when you are doing tasks on small data sets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Properties of word embeddings\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. by taking differences between word embeddings, you can get a better understanding of words that are related to each other. eg. vector difference between boy and girl is similar to the vector difference between duke and dutchess! Only difference is gender. A high degree of similarity between \n",
    "\n",
    "arg max $sim(e_{w}, e_{boy}-e_{duke}+e_{duchess})$, usually cosine similarity is used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|n|   a   | ...  | cow | duke | ... | zucchini|\n",
    "|--|-----|------|------- |-----| ----|-----|\n",
    "|1 | gender|... | 0.03 | -1 |  0.01 | -0.02|\n",
    "|2 |food | ... | -0.02 | 0.7  | 0.08 |0.99|\n",
    "|... | ... |... | ... | ... | ... | ... | \n",
    "|500| furniture |...  | 0.02 | 0.01| ...|-0.03|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "when you multiply one-hot vector representing \"cow\" with embeddingsmatrix, you will basically just keep the column with the embedding representation for cow! In practice, you'll use an embedding layer that simply pulls out a column, because performing these multiplications is much slower when we have huge vectors with all 0's and just one 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Some examples of language models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3.1 learning word embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You want to predict the next word in a sequence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. A sentence: get the word embedding vector for each word. Embedding vector has a fixed amount of rows (=number of embeddings).\n",
    "\n",
    "2. Feed all of them into a neural  network. Usual practice is to fix the amount of presceding words to, let's say, 5. If embedding has 500 rows, leads to 2500-dimensional feature vector going into the first layer. End of the network is a softmax.\n",
    "\n",
    "other possibilities: last 3 words and 3 words after, last 1 word, nearby 1 word."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3.2 word2vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "simpler way to learn embeddings: **skip-gram model**. Context-target pairs. You randomly pick a word to be context word, and then randomly pick a target word withing a specified window from the context word, let's say, 5 words apart.\n",
    "\n",
    "\"The teacher was writing on the blackboard while the principal walked in\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Context: blackboard \n",
    "\n",
    "target: could be principal, while, blackboard,..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to map our context \"c\" (blackboard) to our target \"t\"\n",
    "\n",
    "The model looks like this (with e the embessing : softmax: \n",
    "$P(t|c) = \\displaystyle \\frac{e^{\\Theta^T_te_c}}{\\sum^{10,000}_{j=1}e^{\\Theta^T_je_c}}$\n",
    "\n",
    "loss function: $\\mathcal L (\\hat y, y ) = -\\sum^{10,000}_{i=1}y_i \\log \\hat y_i$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\Theta_t$ is the parameter associated with output t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.coursera.org/learn/nlp-sequence-models/lecture/8CZiw/word2vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Problem: slow, so hierarchical softmax is suggested!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3.3 Negative sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "word2vec is slow! Try this:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- given a pair of words, try to predict whether or not it is a context target pair (1 or 0). This way, label word combinations with 0 or 1. First actively label pairs of words, then create a supervised learning problem.\n",
    "- logistic regression model: P(y=1|c,t) with c and t the context target pair.\n",
    "- Do a small logistic regression model with about 5-6 content-target pairs (only 1 1, rest is 0 so you deliberately use your negative samples!). Repeat this 10,000 times where each time another content word is chosen. faster than working with a softmax classifier! \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3.3 Glove"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GloVe: less frequently used, but has enthusiast because so simple!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again c,t: use x_{i,j} to define how often 2 words appear close to each other."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Words embeddings applications"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- debias embeddings\n",
    "- classify movie reviews"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
